{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Green data and sustainable ML\n",
    "Explore how to reduce environmental impact of data and ML pipelines.\n",
    "\n",
    "## How to read this notebook\n",
    "Most code cells here are **estimators/simulations** (not precise measurements). They are designed to help you reason about key drivers of emissions:\n",
    "- **Compute time** (how long something runs)\n",
    "- **Power draw** (Watts)\n",
    "- **Grid carbon intensity** (gCO2e per kWh)\n",
    "- **Data center overhead** (PUE)\n",
    "- **Data volume** (storage + transfer)\n",
    "\n",
    "Each snippet prints a small \"report\" so you can immediately see what changes when you tweak parameters.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports\n",
    "import time\n",
    "from functools import lru_cache\n",
    "from pprint import pprint\n",
    "from typing import TypedDict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Learning goals\n",
    "- Map the data lifecycle to energy drivers (storage, transfer, compute).\n",
    "- Estimate carbon intensity of workloads and spot hotspots.\n",
    "- Apply efficiency patterns: pruning data, batching, caching, lower precision, and scheduling.\n",
    "- Compare cloud regions by carbon intensity when choosing deployment.\n",
    "\n",
    "### Why this matters\n",
    "Machine learning models and data processing pipelines consume significant amounts of electricity. This energy use translates into carbon emissions depending on the **carbon intensity** of the electricity grid (which varies by region and time). By optimizing code and infrastructure choices, you can often reduce environmental footprint without sacrificing performance. This notebook focuses on practical, code-level changes you can make today."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Quick reference & vocabulary\n",
    "- **PUE (Power Usage Effectiveness):** total facility energy / IT equipment energy. Ideal is 1.0.\n",
    "- **CI (Carbon Intensity):** gCO2e per kWh of electricity. Varies by region and time.\n",
    "- **SCI (Software Carbon Intensity):** rate of carbon emissions per functional unit $R$. $SCI = ((E \\cdot I) + M) / R$.\n",
    "- **Embodied carbon:** CO2e emitted during hardware manufacturing and disposal.\n",
    "- **Operational carbon:** CO2e emitted from running the software (electricity).\n",
    "- **Demand shifting:** moving workloads to times (temporal) or regions (spatial) with lower CI."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8b876a4",
   "metadata": {},
   "source": [
    "## Main Green AI concepts (with sources)\n",
    "- **Green Software Foundation Principles:** Energy efficiency, Hardware efficiency, Carbon awareness. [GSF](https://greensoftware.foundation/)\n",
    "- **Energy Considerations (Strubell et al.):** Training a large model can emit as much carbon as 5 cars in their lifetimes. [Paper](https://arxiv.org/abs/1906.02243)\n",
    "- **Sustainable AI:** Trade-offs between accuracy and efficiency. \"Red AI\" (buying accuracy with massive compute) vs \"Green AI\"."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Green patterns in code\n",
    "- **Carbon awareness:** run jobs when the grid is greener (or choose a lower-CI region when allowed).\n",
    "- **Energy efficiency:** optimize code to run faster or on less hardware.\n",
    "- **Hardware efficiency:** use higher utilization or specialized hardware (GPU/TPU/NPU) when appropriate.\n",
    "- **Data efficiency:** train on less data (curation, deduplication).\n",
    "The code snippets below show concrete examples of applying these patterns."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc7c9412",
   "metadata": {},
   "source": [
    "### How the carbon estimate works (units + formula)\n",
    "The first estimator uses a simple chain of conversions:\n",
    "- Power in **Watts (W)** × time in **hours (h)** → energy in **Watt-hours (Wh)**\n",
    "- Divide by 1000 → **kWh**\n",
    "- Multiply by grid **carbon intensity** (gCO2e/kWh) → **grams of CO2e**\n",
    "\n",
    "In symbols:\n",
    "$$\n",
    "E_{kWh} = \\frac{hours \\cdot watts}{1000}, \\qquad CO2e_{g} = E_{kWh} \\cdot CI\n",
    "$$\n",
    "Assumptions: CPU-only, constant watts, constant CI, no PUE yet (we add that later).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Impact Report: Daily ETL (Coal Region) ---\n",
      "Duration: 12 hours\n",
      "Power: 80 Watts\n",
      "Grid Intensity: 450 gCO2e/kWh\n",
      "Total Energy: 0.96 kWh\n",
      "Total Emissions: 432.00 gCO2e\n",
      "--------------------------------\n"
     ]
    }
   ],
   "source": [
    "# Simple carbon estimate for a data job\n",
    "def estimate_job_impact(job_name: str, cpu_hours: float, watts: float = 65, carbon_intensity: float = 300):\n",
    "    kwh_used = cpu_hours * watts / 1000\n",
    "    co2e_grams = kwh_used * carbon_intensity\n",
    "    \n",
    "    print(f\"--- Impact Report: {job_name} ---\")\n",
    "    print(f\"Duration: {cpu_hours} hours\")\n",
    "    print(f\"Power: {watts} Watts\")\n",
    "    print(f\"Grid Intensity: {carbon_intensity} gCO2e/kWh\")\n",
    "    print(f\"Total Energy: {kwh_used:.2f} kWh\")\n",
    "    print(f\"Total Emissions: {co2e_grams:.2f} gCO2e\")\n",
    "    print(\"--------------------------------\")\n",
    "    return co2e_grams\n",
    "\n",
    "# Scenario: ETL job running for 12 hours on a standard server (80W) in a coal-heavy region (450g)\n",
    "job_co2e = estimate_job_impact(\"Daily ETL (Coal Region)\", cpu_hours=12, watts=80, carbon_intensity=450)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7b8122a6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating regions: us-east-1, eu-west-3, eu-north-1, ap-south-1\n",
      "\n",
      "[BEST CHOICE] Stockholm (Hydro/Wind)\n",
      "   Intensity: 15.0 gCO2e/kWh\n",
      "   PUE: 1.1\n",
      "\n",
      "[AVOID] Mumbai (Coal) (720.0 gCO2e/kWh)\n",
      "[POTENTIAL SAVINGS] 97.9% reduction in operational carbon.\n"
     ]
    }
   ],
   "source": [
    "# Carbon Awareness: Spatial shifting to greener regions\n",
    "# Different regions have different energy mixes (Coal vs. Hydro/Nuclear).\n",
    "# We also consider PUE (Power Usage Effectiveness), which measures data center efficiency.\n",
    "# A PUE of 1.2 means for every 1.0 kWh used by servers, 0.2 kWh is used for cooling/lighting.\n",
    "class RegionInfo(TypedDict):\n",
    "    ci: float\n",
    "    name: str\n",
    "    pue: float\n",
    "regions: dict[str, RegionInfo] = {\n",
    "    'us-east-1': {'ci': 480.0, 'name': 'N. Virginia (Coal/Gas)', 'pue': 1.2},\n",
    "    'eu-north-1': {'ci': 15.0, 'name': 'Stockholm (Hydro/Wind)', 'pue': 1.1},\n",
    "    'eu-west-3': {'ci': 55.0, 'name': 'Paris (Nuclear)', 'pue': 1.15},\n",
    "    'ap-south-1': {'ci': 720.0, 'name': 'Mumbai (Coal)', 'pue': 1.25},\n",
    "}\n",
    "def pick_greenest_region(options: list[str]):\n",
    "    print(f\"Evaluating regions: {', '.join(options)}\")\n",
    "    # Sort by Carbon Intensity (CI)\n",
    "    best = min(options, key=lambda r: regions[r]['ci'])\n",
    "    worst = max(options, key=lambda r: regions[r]['ci'])\n",
    "    \n",
    "    savings = regions[worst]['ci'] - regions[best]['ci']\n",
    "    pct_savings = (savings / regions[worst]['ci']) * 100\n",
    "    \n",
    "    print(f\"\\n[BEST CHOICE] {regions[best]['name']}\")\n",
    "    print(f\"   Intensity: {regions[best]['ci']} gCO2e/kWh\")\n",
    "    print(f\"   PUE: {regions[best]['pue']}\")\n",
    "    print(f\"\\n[AVOID] {regions[worst]['name']} ({regions[worst]['ci']} gCO2e/kWh)\")\n",
    "    print(f\"[POTENTIAL SAVINGS] {pct_savings:.1f}% reduction in operational carbon.\")\n",
    "\n",
    "pick_greenest_region(['us-east-1', 'eu-west-3', 'eu-north-1', 'ap-south-1'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66bae3b2",
   "metadata": {},
   "source": [
    "**What this code does (region picker):** Chooses the region with the lowest carbon intensity (CI) from a list of allowed regions.\n",
    "\n",
    "Step-by-step:\n",
    "1. `regions` is a lookup table: each region has a **carbon intensity** (`ci`) and **PUE** (`pue`).\n",
    "2. `pick_greenest_region(options)` finds the *minimum CI* as the \"best\" and the *maximum CI* as the \"worst\".\n",
    "3. It prints a rough \"percent savings\" based on CI alone.\n",
    "\n",
    "Limitations / real-world caveats:\n",
    "- CI changes over time; in reality you'd use time-aware data (and possibly **temporal shifting**).\n",
    "- You may be constrained by **data residency**, latency, service availability, and cost.\n",
    "- If you want to include PUE in the comparison, compare `ci * pue` instead of `ci`.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Challenge: Lower the estimate\n",
    "Try to lower the estimate: reduce CPU hours (optimize queries), lower watts (right-size instances), or pick a region with lower carbon intensity."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a0c2a3a",
   "metadata": {},
   "source": [
    "### Why \"deduplicate + prune columns\" helps\n",
    "This next code cell demonstrates **data efficiency**: reduce the size of your dataset before you store it, move it, or train on it.\n",
    "\n",
    "What to look for in the output:\n",
    "- `Deduplicated count`: fewer rows → less storage + less downstream compute.\n",
    "- `Kept columns`: fewer columns → smaller records → less transfer and memory usage.\n",
    "- `[ROW REDUCTION]`: a simple percentage reduction indicator.\n",
    "\n",
    "Note: this example deduplicates by raw email for simplicity. In real systems you would often deduplicate by a stable user ID or a pseudonymized identifier (and consider GDPR minimization/purpose limits).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing 5 records...\n",
      "Deduplicated count: 3\n",
      "Kept columns: ['id', 'email', 'country']\n",
      "[ROW REDUCTION] 40.0%\n",
      "[{'country': 'FR', 'email': 'ana@example.com', 'id': 1},\n",
      " {'country': 'DE', 'email': 'lee@example.com', 'id': 3},\n",
      " {'country': 'US', 'email': 'joe@example.com', 'id': 4}]\n"
     ]
    }
   ],
   "source": [
    "# Deduplicate and prune columns to reduce storage and transfer\n",
    "# Expanded dataset simulating raw logs\n",
    "records = [\n",
    "    {'id': 1, 'email': 'ana@example.com', 'country': 'FR', 'timestamp': '2023-10-01T10:00:00', 'browser': 'Chrome', 'os': 'Mac'},\n",
    "    {'id': 2, 'email': 'ana@example.com', 'country': 'FR', 'timestamp': '2023-10-01T10:05:00', 'browser': 'Chrome', 'os': 'Mac'}, # Duplicate user\n",
    "    {'id': 3, 'email': 'lee@example.com', 'country': 'DE', 'timestamp': '2023-10-01T10:10:00', 'browser': 'Firefox', 'os': 'Linux'},\n",
    "    {'id': 4, 'email': 'joe@example.com', 'country': 'US', 'timestamp': '2023-10-01T10:15:00', 'browser': 'Safari', 'os': 'iOS'},\n",
    "    {'id': 5, 'email': 'lee@example.com', 'country': 'DE', 'timestamp': '2023-10-01T10:20:00', 'browser': 'Firefox', 'os': 'Linux'}, # Duplicate user\n",
    "]\n",
    "\n",
    "def optimize_dataset(rows, key, keep_columns):\n",
    "    initial_count = len(rows)\n",
    "    seen = set()\n",
    "    result = []\n",
    "    \n",
    "    print(f\"Processing {initial_count} records...\")\n",
    "    \n",
    "    for row in rows:\n",
    "        k = row[key]\n",
    "        if k in seen:\n",
    "            continue\n",
    "        seen.add(k)\n",
    "        # Pruning: create a new dict with only needed columns\n",
    "        # This reduces memory usage and network transfer size\n",
    "        pruned_row = {col: row[col] for col in keep_columns if col in row}\n",
    "        result.append(pruned_row)\n",
    "        \n",
    "    final_count = len(result)\n",
    "    reduction = 100 * (1 - final_count/initial_count)\n",
    "    \n",
    "    print(f\"Deduplicated count: {final_count}\")\n",
    "    print(f\"Kept columns: {keep_columns}\")\n",
    "    print(f\"[ROW REDUCTION] {reduction:.1f}%\")\n",
    "    return result\n",
    "\n",
    "optimized = optimize_dataset(records, key='email', keep_columns=['id', 'email', 'country'])\n",
    "pprint(optimized)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c98694bc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'current_impact': 'High (Coal region, full precision, hot storage)',\n",
       " 'optimization_1': 'Region shift to...',\n",
       " 'optimization_2': 'Data pruning...',\n",
       " 'optimization_3': 'Lifecycle policy...',\n",
       " 'estimated_savings': '?? %'}"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Write your notes here\n",
    "green_analysis = {\n",
    "    'current_impact': 'High (Coal region, full precision, hot storage)',\n",
    "    'optimization_1': 'Region shift to...',\n",
    "    'optimization_2': 'Data pruning...',\n",
    "    'optimization_3': 'Lifecycle policy...',\n",
    "    'estimated_savings': '?? %'\n",
    "}\n",
    "green_analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fda20d43",
   "metadata": {},
   "source": [
    "**What this code does:** Provides a small structured “worksheet” (`green_analysis`) so you can write down:\n",
    "- what the current footprint drivers are (region/time/storage/precision),\n",
    "- which optimizations you propose (region shift, pruning, lifecycle policies, caching, etc.), and\n",
    "- a rough savings estimate.\n",
    "\n",
    "Tip: try filling it in *after* you run the earlier cells, using the printed numbers as evidence.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b84550f5",
   "metadata": {},
   "source": [
    "## Mini-case: Audit this pipeline\n",
    "- **Scenario:** A daily ETL job runs in `us-east-1` (coal-heavy) at 10 AM (peak load). It processes 1TB of raw logs, keeping all columns, and stores the result in hot storage forever.\n",
    "- **Issues:** High CI region, peak time (no temporal shifting), data bloat (no minimization), storage waste (no lifecycle).\n",
    "- **Fixes:** Move to `eu-north-1` or run at night? Prune columns? Archive old logs?\n",
    "Document your reasoning in the cell below."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb1f98d4",
   "metadata": {},
   "source": [
    "### Quantization / lower precision (why it matters)\n",
    "Model parameters take memory. Memory affects:\n",
    "- how many GPUs/CPUs you need,\n",
    "- how much energy you burn moving weights around, and\n",
    "- whether you can serve efficiently.\n",
    "\n",
    "The next cell is a **back-of-the-envelope** calculation: parameters × bytes-per-parameter → total bytes → GB. It then reports % savings relative to float32.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "dba95f72",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Memory Footprint for 7.0B Parameter Model ---\n",
      "float32 (Full)       |  26.08 GB | Savings:   0.0%\n",
      "float16 (Half)       |  13.04 GB | Savings:  50.0%\n",
      "bfloat16 (Brain float) |  13.04 GB | Savings:  50.0%\n",
      "int8 (Quantized)     |   6.52 GB | Savings:  75.0%\n",
      "int4 (Aggressive)    |   3.26 GB | Savings:  87.5%\n"
     ]
    }
   ],
   "source": [
    "# Hardware Efficiency: Quantization / Lower Precision\n",
    "# Simulating model weights size reduction for a large language model (e.g., 7B parameters)\n",
    "model_params = 7_000_000_000\n",
    "\n",
    "def estimate_model_footprint(params: int):\n",
    "    precisions = {\n",
    "        'float32 (Full)': 4,\n",
    "        'float16 (Half)': 2,\n",
    "        'bfloat16 (Brain float)': 2,\n",
    "        'int8 (Quantized)': 1,\n",
    "        'int4 (Aggressive)': 0.5\n",
    "    }\n",
    "    \n",
    "    print(f\"--- Memory Footprint for {params/1e9:.1f}B Parameter Model ---\")\n",
    "    base_size = params * precisions['float32 (Full)'] / (1024**3) # GB\n",
    "    \n",
    "    for name, bytes_per_param in precisions.items():\n",
    "        size_gb = (params * bytes_per_param) / (1024**3)\n",
    "        savings = 100 * (1 - size_gb/base_size)\n",
    "        print(f\"{name:<20} | {size_gb:>6.2f} GB | Savings: {savings:>5.1f}%\")\n",
    "\n",
    "estimate_model_footprint(model_params)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca254fab",
   "metadata": {},
   "source": [
    "**What this code does:** Estimates how much memory a large model would need under different numeric precisions (float32/float16/int8/int4).\n",
    "- It assumes a fixed number of parameters (here: 7B).\n",
    "- It assigns a simplified “bytes per parameter” for each precision.\n",
    "- It prints the resulting GB and the % savings versus float32.\n",
    "\n",
    "How to interpret it: this is a proxy for potential **hardware + energy** savings (smaller weights often mean faster inference and/or fewer devices). Actual feasibility depends on model architecture and acceptable accuracy loss.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercises\n",
    "1) Extend the carbon estimate to include PUE (multiply kWh by PUE).\n",
    "2) Add caching to a data transform you commonly run and measure runtime savings.\n",
    "3) Draft a lifecycle policy for cold/archival storage and estimate yearly kWh saved.\n",
    "4) Identify constraints (legal or business) that may prevent region-hopping for greener grids."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e7a4ab6",
   "metadata": {},
   "source": [
    "## Solutions\n",
    "\n",
    "The following cells contain example solutions for the exercises above."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c826f39b",
   "metadata": {},
   "source": [
    "### Solution 1: Carbon estimate with PUE\n",
    "\n",
    "PUE (Power Usage Effectiveness) accounts for the overhead of the data center (cooling, lighting).\n",
    "\n",
    "```Total Energy = IT Energy * PUE```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ceb07919",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Impact Report (with PUE 1.2) ---\n",
      "IT Energy: 0.96 kWh\n",
      "Total Facility Energy: 1.15 kWh\n",
      "Total Emissions: 518.40 gCO2e\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "518.4"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def estimate_job_impact_with_pue(cpu_hours: float, watts: float, carbon_intensity: float, pue: float):\n",
    "    kwh_it = cpu_hours * watts / 1000\n",
    "    kwh_total = kwh_it * pue  # Apply PUE multiplier\n",
    "    co2e_grams = kwh_total * carbon_intensity\n",
    "    \n",
    "    print(f\"--- Impact Report (with PUE {pue}) ---\")\n",
    "    print(f\"IT Energy: {kwh_it:.2f} kWh\")\n",
    "    print(f\"Total Facility Energy: {kwh_total:.2f} kWh\")\n",
    "    print(f\"Total Emissions: {co2e_grams:.2f} gCO2e\")\n",
    "    return co2e_grams\n",
    "\n",
    "estimate_job_impact_with_pue(cpu_hours=12, watts=80, carbon_intensity=450, pue=1.2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e6f6eb4",
   "metadata": {},
   "source": [
    "### Solution 2: Caching to reduce redundant compute\n",
    "We use Python's built-in `lru_cache` to avoid re-running the same expensive transformation for the same inputs.\n",
    "\n",
    "What to look for:\n",
    "- The **first run** includes cache misses → slower.\n",
    "- The **second run** reuses cached results → faster.\n",
    "\n",
    "Why it's \"green\": fewer repeated computations generally means less CPU time → less energy use. (This is especially true for batch ETL transforms and feature engineering pipelines.)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "2bb65b7b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First run (some cache misses):\n",
      "Processed 8 items in 2.52 seconds\n",
      "\n",
      "Second run (all cache hits):\n",
      "Processed 8 items in 0.00 seconds\n"
     ]
    }
   ],
   "source": [
    "# Simulate an expensive data transformation\n",
    "@lru_cache(maxsize=128)\n",
    "def expensive_transform(data_id):\n",
    "    time.sleep(0.5)  # Simulate 500ms of heavy compute\n",
    "    return f\"Transformed_{data_id}\"\n",
    "\n",
    "def run_pipeline(ids):\n",
    "    start = time.time()\n",
    "    for i in ids:\n",
    "        _ = expensive_transform(i)\n",
    "    end = time.time()\n",
    "    print(f\"Processed {len(ids)} items in {end - start:.2f} seconds\")\n",
    "\n",
    "data_ids = [1, 2, 1, 3, 2, 4, 1, 5]  # Note duplicates: 1 and 2 appear multiple times\n",
    "\n",
    "print(\"First run (some cache misses):\")\n",
    "run_pipeline(data_ids)\n",
    "\n",
    "print(\"\\nSecond run (all cache hits):\")\n",
    "run_pipeline(data_ids)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e24a2893",
   "metadata": {},
   "source": [
    "### Solution 3: Lifecycle Policy Draft\n",
    "\n",
    "**Goal:** Reduce storage energy for data that is rarely accessed.\n",
    "\n",
    "**Policy:**\n",
    "1.  **Hot Storage (SSD/Standard S3):** Data < 30 days old. High availability, high energy cost.\n",
    "2.  **Warm Storage (HDD/Infrequent Access):** Data 30-90 days old. Lower availability, lower energy.\n",
    "3.  **Cold Storage (Tape/Glacier):** Data > 90 days old. Very low energy (often offline), high retrieval latency.\n",
    "4.  **Delete:** Logs > 1 year old (unless required for compliance).\n",
    "\n",
    "**Estimated Savings:**\n",
    "Moving 1TB of data from Standard (Hot) to Archive (Cold) can reduce associated carbon emissions by **~80-90%** due to the passive nature of tape storage compared to spinning disks or SSDs.\n",
    "\n",
    "---\n",
    "\n",
    "### Solution 4: Constraints on Region Hopping\n",
    "\n",
    "While moving workloads to Sweden (eu-north-1) or France (eu-west-3) might be greener, you may be blocked by:\n",
    "\n",
    "1.  **Data Residency Laws (GDPR):** Personal data of EU citizens generally shouldn't leave the EU. Data of US citizens might need to stay in the US depending on specific sector regulations (e.g., health data).\n",
    "2.  **Latency:** If your users are in India, serving them from Sweden will introduce significant lag, degrading user experience.\n",
    "3.  **Service Availability:** Not all cloud services (e.g., specific GPU types) are available in every region.\n",
    "4.  **Cost:** Electricity prices vary, and so do cloud prices. Sometimes the greenest region is more expensive."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "aipm-GDPR-greendata-aiact",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
